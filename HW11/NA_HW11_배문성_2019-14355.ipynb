{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8a6184",
   "metadata": {},
   "source": [
    "# Ex\n",
    "### Exercise 13.1\n",
    "- 5(a), 7, 10, 11, 12, 15, 16\n",
    "\n",
    "### Exercise 13.2\n",
    "- 2, 7, 13, 15, 20, 21, 22, 23, 24, 25\n",
    "\n",
    "# Com Ex\n",
    "### Computer Exercise 13.1\n",
    "- 2(a)\n",
    "\n",
    "### Computer Exercise 13.2\n",
    "- 1(a), 1(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8b0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e747e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden Section Search fo Sin(x)\n",
      "x = 4.515392518989481e-11, f(x) = 4.515392518989481e-11\n",
      "Scipy Minimize for Sin(x)\n",
      "x = 5.786836745895891e-06, f(x) = 5.786836745863593e-06\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def golden_section_search(f, a, b, tol=1e-10):\n",
    "    gratio = (np.sqrt(5) - 1) / 2\n",
    "\n",
    "    a1 = b - gratio * (b - a)\n",
    "    a2 = a + gratio * (b - a)\n",
    "    \n",
    "    while abs(b - a) > tol:\n",
    "        func1 = f(a1)\n",
    "        func2 = f(a2)\n",
    "        if func1 > func2:\n",
    "            a = a1\n",
    "            a1 = a2\n",
    "            a2 = a + gratio * (b - a)\n",
    "        else:\n",
    "            b = a2\n",
    "            a2 = a1\n",
    "            a1 = b - gratio * (b - a)\n",
    "        \n",
    "    x_opt = (a + b) / 2\n",
    "    return x_opt\n",
    "\n",
    "print(\"Golden Section Search fo Sin(x)\")\n",
    "x_golden = golden_section_search(np.sin, 0, np.pi/2)\n",
    "print(f\"x = {x_golden}, f(x) = {np.sin(x_golden)}\")\n",
    "\n",
    "print(\"Scipy Minimize for Sin(x)\")\n",
    "x_scipy = sp.optimize.minimize_scalar(np.sin, bounds=(0, np.pi / 2), method='bounded')\n",
    "print(f\"x = {x_scipy.x}, f(x) = {np.sin(x_scipy.x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9be8af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 6\n",
      "Newton's Method for Rosenbrock: [1. 1.]\n",
      "Converged at iteration 67\n",
      "BFGS for Rosenbrock: [1. 1.]\n",
      "Converged at iteration 87\n",
      "Nelder-Mead for Rosenbrock: [1.00082096 1.0016435 ]\n",
      "Temperature below Tmin at iteration 269\n",
      "Simulated Annealing for Rosenbrock: [ 0.1211715  -0.00186836]\n",
      "Scipy Minimize BFGS for Rosenbrock: [0.9999955  0.99999099]\n",
      "Scipy Minimize Nelder-Mead for Rosenbrock: [1.00002202 1.00004222]\n",
      "Scipy Minimize Simulated Annealing for Rosenbrock: [1.00002523 1.00004788]\n",
      "\n",
      "Converged at iteration 13\n",
      "Newton's Method for Woods: [-0.96797404  0.94713917 -0.96951629  0.95124763]\n",
      "Division by zero in BFGS update.\n",
      "BFGS for Woods: [ 3.55250936e-71 -1.40745583e+68  4.15383749e+34  1.56526148e+69]\n",
      "Converged at iteration 11\n",
      "Nelder-Mead for Woods: [0 0 0 0]\n",
      "Temperature below Tmin at iteration 269\n",
      "Simulated Annealing for Woods: [-0.66446579  0.48187861 -1.11412755  1.2783952 ]\n",
      "Scipy Minimize BFGS for Woods: [0.9999998  0.99999962 1.00000008 1.00000017]\n",
      "Scipy Minimize Nelder-Mead for Woods: [0.99999777 0.99999832 1.00000621 1.0000122 ]\n",
      "Scipy Minimize Simulated Annealing for Woods: [-1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "def newton_method(f, grad_f, hess_f, x0, tol=1e-6, max_iter=100):\n",
    "    x = x0.copy()\n",
    "    for i in range(max_iter):\n",
    "        g = grad_f(x)\n",
    "        H = hess_f(x)\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "        try:\n",
    "            p = np.linalg.solve(H, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Hessian is singular or not positive definite.\")\n",
    "            break\n",
    "        x = x - p\n",
    "    return x\n",
    "\n",
    "def bfgs(f, grad_f, x0, tol=1e-6, max_iter=100):\n",
    "    n = len(x0)\n",
    "    x = x0.copy()\n",
    "    B = np.eye(n)\n",
    "    for i in range(max_iter):\n",
    "        g = grad_f(x)\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "\n",
    "        p = -np.linalg.solve(B, g)\n",
    "        alpha = 1.0\n",
    "        x_new = x + alpha * p\n",
    "\n",
    "        s = x_new - x\n",
    "        y = grad_f(x_new) - g\n",
    "        ys = np.dot(y, s)\n",
    "\n",
    "        if ys == 0.0:\n",
    "            print(\"Division by zero in BFGS update.\")\n",
    "            break\n",
    "\n",
    "        Bs = B @ s\n",
    "        B += np.outer(y, y) / ys - np.outer(Bs, Bs) / (s @ Bs)\n",
    "\n",
    "        x = x_new\n",
    "\n",
    "    return x\n",
    "\n",
    "def nelder_mead(f, x0, alpha=1, gamma=2, rho=0.5, sigma=0.5, tol=1e-6, max_iter=200):\n",
    "    n = len(x0)\n",
    "    simplex = [x0]\n",
    "    for i in range(n):\n",
    "        y = x0.copy()\n",
    "        y[i] += 1.0\n",
    "        simplex.append(y)\n",
    "    simplex = np.array(simplex)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        vals = np.array([f(x) for x in simplex])\n",
    "        idx = np.argsort(vals)\n",
    "        simplex = simplex[idx]\n",
    "        vals = vals[idx]\n",
    "\n",
    "        centroid = np.mean(simplex[:-1], axis=0)\n",
    "        xr = centroid + alpha * (centroid - simplex[-1])\n",
    "        fr = f(xr)\n",
    "\n",
    "        if vals[0] <= fr < vals[-2]:\n",
    "            simplex[-1] = xr\n",
    "        elif fr < vals[0]:\n",
    "            xe = centroid + gamma * (xr - centroid)\n",
    "            fe = f(xe)\n",
    "            if fe < fr:\n",
    "                simplex[-1] = xe\n",
    "            else:\n",
    "                simplex[-1] = xr\n",
    "        else:\n",
    "            xc = centroid + rho * (simplex[-1] - centroid)\n",
    "            fc = f(xc)\n",
    "            if fc < vals[-1]:\n",
    "                simplex[-1] = xc\n",
    "            else:\n",
    "                for i in range(1, n+1):\n",
    "                    simplex[i] = simplex[0] + sigma * (simplex[i] - simplex[0])\n",
    "\n",
    "        if np.std(vals) < tol:\n",
    "            print(f\"Converged at iteration {iteration}\")\n",
    "            break\n",
    "\n",
    "    return simplex[0]\n",
    "\n",
    "def simulated_annealing(f, x0, T0=1.0, Tmin=1e-6, alpha=0.95, max_iter=10000, step_size=0.1):\n",
    "    x_curr = x0.copy()\n",
    "    f_curr = f(x_curr)\n",
    "    x_best = x_curr.copy()\n",
    "    f_best = f_curr\n",
    "    T = T0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # 랜덤 이웃점 생성 (여기서는 단순히 각 차원에 step_size 범위 내에서 노이즈 추가)\n",
    "        x_new = x_curr + np.random.uniform(-step_size, step_size, size=len(x0))\n",
    "        f_new = f(x_new)\n",
    "        delta = f_new - f_curr\n",
    "\n",
    "        if delta < 0 or np.random.rand() < np.exp(-delta / T):\n",
    "            x_curr = x_new\n",
    "            f_curr = f_new\n",
    "            if f_new < f_best:\n",
    "                x_best = x_new\n",
    "                f_best = f_new\n",
    "\n",
    "        T *= alpha\n",
    "        if T < Tmin:\n",
    "            print(f\"Temperature below Tmin at iteration {i}\")\n",
    "            break\n",
    "\n",
    "    return x_best\n",
    "\n",
    "rosenbrock = lambda x: sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n",
    "grad_rosenbrock = lambda x: np.array([-400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0]), 200 * (x[1] - x[0]**2)])\n",
    "hess_rosenbrock = lambda x: np.array([[1200 * x[0]**2 - 400 * x[1] + 2, -400 * x[0]], [-400 * x[0], 200]])\n",
    "\n",
    "print(f\"Newton's Method for Rosenbrock: {newton_method(rosenbrock, grad_rosenbrock, hess_rosenbrock, np.array([-1.2, 1]))}\")\n",
    "print(f\"BFGS for Rosenbrock: {bfgs(rosenbrock, grad_rosenbrock, np.array([-1.2, 1]))}\")\n",
    "print(f\"Nelder-Mead for Rosenbrock: {nelder_mead(rosenbrock, np.array([-1.2, 1]))}\")\n",
    "print(f\"Simulated Annealing for Rosenbrock: {simulated_annealing(rosenbrock, np.array([-1.2, 1]))}\")\n",
    "print(f\"Scipy Minimize BFGS for Rosenbrock: {sp.optimize.minimize(rosenbrock, np.array([-1.2, 1]), method='BFGS').x}\")\n",
    "print(f\"Scipy Minimize Nelder-Mead for Rosenbrock: {sp.optimize.minimize(rosenbrock, np.array([-1.2, 1]), method='Nelder-Mead').x}\")\n",
    "print(f\"Scipy Minimize Simulated Annealing for Rosenbrock: {sp.optimize.dual_annealing(rosenbrock, bounds=[(-2, 2), (-1, 3)]).x}\")\n",
    "print()\n",
    "\n",
    "woods = lambda x: (\n",
    "    100 * (x[0] ** 2 - x[1]) ** 2 + (x[0] - 1) ** 2 +\n",
    "    90 * (x[2] ** 2 - x[3]) ** 2 + (x[2] - 1) ** 2 +\n",
    "    10.1 * ((x[1] - 1) ** 2 + (x[3] - 1) ** 2) +\n",
    "    19.8 * (x[1] - 1) * (x[3] - 1)\n",
    ")\n",
    "\n",
    "grad_woods = lambda x: np.array([\n",
    "    400 * x[0] * (x[0]**2 - x[1]) + 2 * (x[0] - 1),\n",
    "    -200 * (x[0]**2 - x[1]) + 20.2 * (x[1] - 1) + 19.8 * (x[3] - 1),\n",
    "    360 * x[2] * (x[2]**2 - x[3]) + 2 * (x[2] - 1),\n",
    "    -180 * (x[2]**2 - x[3]) + 20.2 * (x[3] - 1) + 19.8 * (x[1] - 1)\n",
    "])\n",
    "\n",
    "hess_woods = lambda x: np.array([\n",
    "    [\n",
    "        1200 * x[0]**2 - 400 * x[1] + 2,      \n",
    "        -400 * x[0],                          \n",
    "        0,                                    \n",
    "        0                                     \n",
    "    ],\n",
    "    [\n",
    "        -400 * x[0],                         \n",
    "        220.2,                               \n",
    "        0,\n",
    "        19.8                                 \n",
    "    ],\n",
    "    [\n",
    "        0,\n",
    "        0,\n",
    "        1080 * x[2]**2 - 360 * x[3] + 2,      \n",
    "        -360 * x[2]                           \n",
    "    ],\n",
    "    [\n",
    "        0,\n",
    "        19.8,                                \n",
    "        -360 * x[2],                         \n",
    "        200.2                                \n",
    "    ]\n",
    "])\n",
    "\n",
    "print(f\"Newton's Method for Woods: {newton_method(woods, grad_woods, hess_woods, np.array([-3, -1, -3, -1]))}\")\n",
    "print(f\"BFGS for Woods: {bfgs(woods, grad_woods, np.array([-3, -1, -3, -1]))}\")\n",
    "print(f\"Nelder-Mead for Woods: {nelder_mead(woods, np.array([-3, -1, -3, -1]))}\")\n",
    "print(f\"Simulated Annealing for Woods: {simulated_annealing(woods, np.array([-3, -1, -3, -1]))}\")\n",
    "print(f\"Scipy Minimize BFGS for Woods: {sp.optimize.minimize(woods, np.array([-3, -1, -3, -1]), method='BFGS').x}\")\n",
    "print(f\"Scipy Minimize Nelder-Mead for Woods: {sp.optimize.minimize(woods, np.array([-3, -1, -3, -1]), method='Nelder-Mead').x}\")\n",
    "print(f\"Scipy Minimize Simulated Annealing for Woods: {sp.optimize.dual_annealing(woods, bounds=[(-3, -1), (-3, -1), (-3, -1), (-3, -1)]).x}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
